---
title: "Airflow"
author: "Pierre Formont"
format: 
  revealjs:
    highlight-style: a11y
    code-block-background: true
    code-block-border-left: true
    code-line-numbers: true
    incremental: true
    smaller: true
---

## Agenda

- Airflow, a short introduction
- DAGs
- Tasks
- Exercises

## What is Airflow

- open-source platform for developing and scheduling worfklows
- workflows defined as code in Python:
  - dynamic
  - extensible
  - flexible
- very important piece of the data industry, used in many major companies, _e.g._:
  - Airbnb (where it started in 2014)
  - NASA
  - Paypal
- part of the [Apache Software Foundation](https://apache.org/) since 2016

## Apache Software Foundation

- non-profit organization founded in 1999
- largest open-source foundation
- their goal is to provide "software for the public good"
- many projects in the data world, _e.g._
  - Airflow: orchestrator 
  - Avro: data serialization system
  - Kafka: publish-subscribe messaging system
  - Flink: large-scale data processing
  - Cassandra: large-scale distributed database

## Airflow main concepts

- workflows are collection of **Tasks** to be executed in a specific order
- definition of the workflow represented as a DAG = **`D`**`irected` **`A`**`cyclic` **`G`**`raph`:
  - _graph_: a set of objects -- here **Tasks** -- that are related to each other
  - _directed_: the connections between objects have a direction, _e.g._ we want to execute task A before task B
  - _acyclic_: there is no cycle in the graph, _e.g._ :
    - task B depends on task A
    - task C depends on task B
    - task A cannot depend on task B or C
- a **DagRun** represents one execution of this workflow, _e.g._:
  - a specific DAG is scheduled to run every day at 1am, starting today
  - there will be 1 DagRun today at 1am, 1 DagRun tomorrow at 1am, etc.

## Airflow UI - DAGs page (1/2)

![](resources/images/airflow_ui_dags.png)

- Left toggle: pause / unpause a DAG
- DAG: the name of the DAG + optional tags
- Owner: the name of who is responsible for this DAG
- Runs: count of DagRuns for this DAG, grouped by status
- Schedule: when is this DAG scheduled to run

## Airflow UI - DAGs page (2/2)

![](resources/images/airflow_ui_dags.png)

- Last Run: timestamp of the last DagRun
- Next Run: timestamp of when the next DagRun will occur
- Recent Tasks: count of tasks for the past DagRun, grouped by status
- Actions: `Trigger DAG` and `Delete DAG` button

## Airflow UI - focus on a DAG

![](resources/images/airflow_ui_dags_example.png)

- toggle is blue -> the DAG is unpaused
- the DAG has run once and it was a failure
- there is no Next Run timestamp as the DAG as a `None` Schedule (only run when manually triggered)
- in the past run:
  - 2 tasks succeeded
  - 1 task failed
  - 6 tasks have not run because a task before them failed

## Airflow UI - DAG Graph page

![](resources/images/airflow_ui_dag_graph.png)

- can see all tasks and how they are related
- tasks that succeeded in green, failed in red and could not run -- `upstream_failed` in orange

## Airflow UI - DAG Graph Task page

![](resources/images/airflow_ui_dag_graph_task.png)

- clicking on a task brings out a modal with lots of actions
- in particular, you can see the log for the task to understand what happened

## Airflow UI - Task log page

![](resources/images/airflow_ui_task_log.png)

- here, we can see it's missing a file, raising an Exception

## Airflow UI - DAG code page

![](resources/images/airflow_ui_dag_code.png)

- in this we can see the actual definition of the DAG
- it's Python code !

## Write a DAG

Since Airflow 2.0, DAGs can be written as Python functions with a specific decorator.

```{.python}
from airflow.decorators import dag
from datetime import datetime

@dag(
    schedule="0 0 * * *", # define how often the DAG runs, cron syntax
    start_date=datetime(2023, 1, 11), # define the date of the first execution
    catchup=False # if a run fails, do not try to rerun it
)
def data_structuration_and_transportation():
    # Insert worfklow code here
    pass


_ = data_structuration_and_transportation()
```

Place this file in the `dags` folder of your repository and Airflow will pick it up.

## Write a Task

Two ways to write a Task:

- use one of the `Operators` packaged in Airflow, or provided by a third-party:
  - packaged operators for many use cases:
    - [BashOperator](https://airflow.apache.org/docs/apache-airflow/2.1.0/_api/airflow/operators/bash/index.html#module-airflow.operators.bash): run a shell command
    - [SQLiteOperator](https://airflow.apache.org/docs/apache-airflow-providers-sqlite/stable/_api/airflow/providers/sqlite/operators/sqlite/index.html#module-airflow.providers.sqlite.operators.sqlite): execute SQL code in a specific sqlite database
    - [DockerOperator](https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html#): execute a command inside a running Docker container
  - all Cloud vendors propose Python packages to interact with their services from Airflow, _e.g._
      - [AWS](https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/index.html)
      - [GCP](https://airflow.apache.org/docs/apache-airflow-providers-google/stable/index.html)
- since Airflow 2.0, the Taskflow API defines the `@task` decorator to abstract away most of the boilerplate if using only Python code

## Hello world

```{.python}
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from datetime import datetime

@dag(
    schedule=None, # only triggered manually
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_operator_dag():
    
    def say_hello() -> str:
      return "Hello, world!"
    
    PythonOperator(task_id="hello", python_callable=say_hello)

_ = hello_operator_dag()
```

## Hello world

```{.python code-line-numbers="12-13"}
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from datetime import datetime

@dag(
    schedule=None, # only triggered manually
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_operator_dag():
    
    def say_hello() -> str:
      return "Hello, world!"
    
    PythonOperator(task_id="hello", python_callable=say_hello)

_ = hello_operator_dag()
```

Define the Python function to be called

## Hello world

```{.python code-line-numbers="15"}
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from datetime import datetime

@dag(
    schedule=None, # only triggered manually
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_operator_dag():
    
    def say_hello() -> str:
      return "Hello, world!"
    
    PythonOperator(task_id="hello", python_callable=say_hello)

_ = hello_operator_dag()
```

Create a Task of type PythonOperator with:

- `task_id="hello"`: the name of the task, will be seen in the UI, should be unique to a DAG
- `python_callable=say_hello`: tell the operator to run this Python function

## Hello world output

![](resources/images/airflow_ui_hello_log.png)

## Hello world

Using the `@task` decorator, Airflow tasks are simple Python functions, _e.g._

```{.python}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def data_structuration_and_transportation():
    
    @task
    def say_hello() -> str:
      return "Hello, world!"

    hello = say_hello()

_ = data_structuration_and_transportation()
```

## Hello world

Define the task.

Note that there is no need to define the `task_id`, it is picked up automatically from the function name.

```{.python code-line-numbers="11-13"}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None, # only triggered manually
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def data_structuration_and_transportation():
    
    @task
    def say_hello() -> str:
      return "Hello, world!"

    hello = say_hello()

_ = data_structuration_and_transportation()
```

## Hello world

Call the task to be instantiated

```{.python code-line-numbers="15"}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None, # only triggered manually
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def data_structuration_and_transportation():
    
    @task
    def say_hello() -> str:
      return "Hello, world!"

    hello = say_hello()

_ = data_structuration_and_transportation()
```

## Hello world output

![](resources/images/airflow_ui_hello_log.png)

## Using parameters in tasks

```{.python}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_with_parameters_dag():
    
    @task
    def say_hello(name: str) -> str:
      return f"Hello, world! My name is {name}"

    hello = say_hello("Pierre")

_ = hello_with_parameters_dag()
```

## Create relations between tasks

Relations between tasks can be set using the bitshift operator `>>`.

```{.python}
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_operator_with_relations_dag():
    
    def say_hello() -> str:
      return "Hello, world!"
    
    hello = PythonOperator(task_id="hello", python_callable=say_hello)

    hello_again = PythonOperator(task_id="hello_again", python_callable=say_hello)

    hello >> hello_again

_ = hello_operator_with_relations_dag()
```

## Create relations between tasks

Relations between tasks can be set using the bitshift operator `>>`.

```{.python code-line-numbers="19"}
from airflow.decorators import dag
from airflow.operators.python import PythonOperator
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_operator_with_relations_dag():
    
    def say_hello() -> str:
      return "Hello, world!"
    
    hello = PythonOperator(task_id="hello", python_callable=say_hello)

    hello_again = PythonOperator(task_id="hello_again", python_callable=say_hello)

    hello >> hello_again

_ = hello_operator_with_relations_dag()
```

## Hello task instance details

In the [task instance details](http://localhost:8080/task?dag_id=hello_operator_with_relations_dag&task_id=hello&execution_date=2023-01-12T14%3A39%3A59.802469%2B00%3A00), we can see the `hello_again` task is `downstream` of the `hello` task, _i.e_ it depends on the `hello` task to succeed before it runs.

![](resources/images/airflow_ui_hello_downstream.png)

## Pass data between tasks

Now we know how to use parameters in tasks, we can use the results from a previous task to parameterize its downstream tasks.

```{.python}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_with_passing_data_dag():

    @task
    def give_name() -> str:
      return "Pierre"
    
    @task
    def say_hello(name: str) -> str:
      return f"Hello, world! My name is {name}"

    name = give_name()
    hello = say_hello(name)

_ = hello_with_passing_data_dag()
```

## Pass data between tasks

Now we know how to use parameters in tasks, we can use the results from a previous task to parameterize its downstream tasks.

```{.python code-line-numbers="20"}
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_with_passing_data_dag():

    @task
    def give_name() -> str:
      return "Pierre"
    
    @task
    def say_hello(name: str) -> str:
      return f"Hello, world! My name is {name}"

    name = give_name()
    hello = say_hello(name)

_ = hello_with_passing_data_dag()
```

This construct passes the value of the `give_name` task to the `say_hello` task and automatically creates the dependency !

## Exercises

**Exercise 15**

> Create a DAG with two tasks: one task that reads the `users.json` file and outputs it as stringified json, and one task that reads this string, transforms it into a list of dataclass instances, prints them and returns nothing

. . .

Hints:

- copy the `users.json` file in the `dags` directory so it can be picked up by Airflow
- use `json.dumps` to stringify the user list (a Task can only return a single value, this is why we don't return the list directly)
- use `json.loads` to read the stringified list back into a Python dictionary

## Multiple outputs

It is actually possible to return multiple values from an Airflow task using the `multiple_outputs` parameter to the `@task` decorator

. . .

```{.python}
from typing import Dict
from airflow.decorators import dag, task
from datetime import datetime

@dag(
    schedule=None,
    start_date=datetime(2023, 1, 11),
    catchup=False
)
def hello_with_multiple_outputs_dag():

    @task(multiple_outputs=True)
    def give_names() -> Dict:
      return {"names": ["Pierre", "Ada"]}
    
    @task
    def say_hello(names: Dict):
      for name in names["names"]:
        print(name)

    names = give_names()
    hello = say_hello(names)

_ = hello_with_multiple_outputs_dag()
```

## Multiple outputs


**Exercise 16**

> Rework the previous DAG to return the list of users in a dictionary instead of a string in the first task, and adapt the second task for this pattern


## Connections

- declare connections to other services that can be used in every DAG
- example using the [SqliteOperator](https://airflow.apache.org/docs/apache-airflow-providers-sqlite/stable/operators.html#using-the-operator)

. . .

![](resources/images/airflow_create_connection.png)

## Connections

**Exercise 17**

> Declare a connection for a SQLite database and use the SqliteOperator to insert one row of data

## Hooks

- using the Operator, only one statement can be processed at a time so only one row can be inserted
- we can use the [Hook](https://airflow.apache.org/docs/apache-airflow-providers-sqlite/stable/_api/airflow/providers/sqlite/hooks/sqlite/index.html), the actual interface to `sqlite` on which the Operator is built upon,
- it is based on a common hook for [database operations](https://airflow.apache.org/docs/apache-airflow-providers-common-sql/stable/_api/airflow/providers/common/sql/hooks/sql/index.html), which declares a lot of useful methods, _e.g._ `insert_rows`

. . .

```{.python}
def insert_with_hook():
    sqlite_hook = SqliteHook()
    rows = [("0001", "Pierre", "Paris", "EPITA" , 36, 1), ("0002", "Ada", "Boston", "Home", 27, 0)]
    target_fields = ['id', 'name', 'city', 'school', 'age', 'is_teacher']
    sqlite_hook.insert_rows(table='users', rows=rows, target_fields=target_fields)
```


**Exercise 18**

> Replace the add_data operator from exercise17 with the `insert_with_hook` task